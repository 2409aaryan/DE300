{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Task 1"
      ],
      "metadata": {
        "id": "Swtm0xz71bSx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Cc5wVVzu0ECD",
        "outputId": "5b1b4c1c-57fb-4b70-bb22-0e672b15149d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  21.3M      0  0:00:01  0:00:01 --:--:-- 21.3M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))\n",
        "\n",
        "if 'id' not in agnews.columns and '_c0' in agnews.columns:\n",
        "    agnews = agnews.withColumnRenamed('_c0', 'id')\n"
      ],
      "metadata": {
        "id": "3tBDERgM0enn"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "m2G5aMjp0uJP",
        "outputId": "b2428c49-e3b2-4bf0-948b-4a3bd63dea18"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "| id|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# TF Function\n",
        "def tf_mapper(df):\n",
        "    \"\"\"Explode tokens → TF per (doc, term).\"\"\"\n",
        "    exploded  = df.select('id', F.explode('filtered').alias('term'))\n",
        "    term_freq = exploded.groupBy('id', 'term').count() \\\n",
        "                        .withColumnRenamed('count', 'term_count')\n",
        "    doc_len   = exploded.groupBy('id').count() \\\n",
        "                        .withColumnRenamed('count', 'doc_len')\n",
        "    return term_freq.join(doc_len, 'id') \\\n",
        "                    .withColumn('tf', F.col('term_count') / F.col('doc_len'))\n",
        "\n",
        "\n",
        "# IDF Function\n",
        "def idf_mapper(tf_df, n_docs):\n",
        "    \"\"\"IDF per term.\"\"\"\n",
        "    docs_with_term = (tf_df.select('term', 'id').distinct()\n",
        "                               .groupBy('term').count()\n",
        "                               .withColumnRenamed('count', 'docs_with_term'))\n",
        "    return docs_with_term.withColumn(\n",
        "        'idf', F.log(F.lit(n_docs) / F.col('docs_with_term'))\n",
        "    )\n",
        "\n",
        "\n",
        "# TF-IDF Function\n",
        "def tfidf_reduce(tf_df, idf_df):\n",
        "    \"\"\"Join TF & IDF → TF-IDF.\"\"\"\n",
        "    return (tf_df.join(idf_df.select('term', 'idf'), 'term')\n",
        "                 .withColumn('tfidf', F.col('tf') * F.col('idf'))\n",
        "                 .select('id', 'term', 'tfidf'))\n"
      ],
      "metadata": {
        "id": "5GlWivug1gIx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "# Calculate tf-idf measure for each row\n",
        "total_docs = agnews.count()\n",
        "\n",
        "tf_df   = tf_mapper(agnews)\n",
        "idf_df  = idf_mapper(tf_df, total_docs)\n",
        "tfidf_df = tfidf_reduce(tf_df, idf_df)\n",
        "\n",
        "tfidf_map_df = (tfidf_df.groupBy('id')\n",
        "                           .agg(F.map_from_entries(\n",
        "                                   F.collect_list(F.struct('term', 'tfidf'))\n",
        "                               ).alias('tfidf')))\n",
        "\n",
        "agnews = agnews.join(tfidf_map_df, 'id')   # ← adds the new “tfidf” column\n"
      ],
      "metadata": {
        "id": "GhgfkkP_2a7p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "# Print out the tf-idf measure for the first 5 documents.\n",
        "agnews.select('id', 'tfidf').show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Y7Sxx2eh20m_",
        "outputId": "acba461c-3191-4f5b-aa63-95270eecd6f5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|id |tfidf                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|1  |{investment -> 0.1890771769001148, commercial -> 0.2057832028092643, reputation -> 0.2578098186776328, group -> 0.12468100563149095, plays -> 0.22418048797172685, reuters -> 0.1650267812443044, firm -> 0.15969712503706046, private -> 0.1929050573011279, occasionally -> 0.33274321954270536, market -> 0.13394932212703356, part -> 0.16022031730914288, bets -> 0.27861293130724324, placed -> 0.2284965552404658, looks -> 0.1973537176743789, quietly -> 0.25188254045524316, making -> 0.1698717076460444, industry -> 0.15043731768548949, aerospace -> 0.2581171817448437, toward -> 0.1898997183872362, carlyle -> 0.7168306746824437, timed -> 0.324478643568105, defense -> 0.1751279339938823, well -> 0.17053284421704767, another -> 0.14507889141437585, controversial -> 0.20949395177306526}|\n",
            "|3  |{exports -> 0.2146590164054526, infrastructure -> 0.22959926718225876, reuters -> 0.15913296762843637, militia -> 0.2252006141545402, official -> 0.15149485319300557, halts -> 0.27365396741681164, said -> 0.06593367258642661, southern -> 0.336553609483104, rebel -> 0.18209445014364567, oil -> 0.35763832555989516, showed -> 0.1743365558077232, pipeline -> 0.4720829409342409, iraq -> 0.23809526243476142, authorities -> 0.18159366801541998, export -> 0.23862435123782139, main -> 0.36492623402353547, flows -> 0.2774168429760197, saturday -> 0.12197305137253434, intelligence -> 0.20782569445751425, halted -> 0.2557691357056513, strike -> 0.17411586950893898}                                                                                                                            |\n",
            "|5  |{positive -> 0.18127557126337487, 46 -> 0.2067185029184427, o -> 0.1405921241478995, end -> 0.1131018693698805, year -> 0.16492932872045324, friday -> 0.09051819454875144, outlook -> 0.15994024564769707, computer -> 0.12125053715366763, 36 -> 0.145856640464541, barrel -> 0.15591601639460734, inc -> 0.0925193542091254, maker -> 0.12413070044238618, surged -> 0.19042797405490253, slightly -> 0.17676052180852383, stocks -> 0.22465153652572792, prices -> 0.10854419401585633, dell -> 0.35254772027561204, ended -> 0.15552292945064294, reuters -> 0.13924134667488183, past -> 0.13552111644471848, lows -> 0.3919911697751357, offsetting -> 0.27225416288029386, higher -> 0.13245598339561715, near -> 0.27092999101551124, oil -> 0.10431117828830276, stayed -> 0.21601464260731984}        |\n",
            "|6  |{98 -> 0.24380014644675033, billion -> 0.12463394966614495, money -> 0.32032556569959436, thursday -> 0.09411730353602658, assets -> 0.19828377199707944, nation -> 0.14600491745222022, retail -> 0.1756323877136072, said -> 0.06153809441399818, week -> 0.20995041270602935, market -> 0.1205543899143302, 1 -> 0.09785536680483083, trillion -> 0.25173210101672866, investment -> 0.17016945921010332, mutual -> 0.2034722156541392, 17 -> 0.1550563400540995, institute -> 0.21681274072820655, latest -> 0.28333759132146574, ap -> 0.17648274643377362, fell -> 0.30169124303768075, 849 -> 0.35526811204082387, 36 -> 0.3111608329910208, funds -> 0.3773887391640401, company -> 0.09509468127233014}                                                                                                 |\n",
            "|9  |{wall -> 0.48467229409055657, green -> 0.27256812064061997, ultra -> 0.3908380162950787, new -> 0.10037349680959233, claw -> 0.47284562777121286, st -> 0.2448690293108052, band -> 0.3451662430856316, black -> 0.2797741636452582, york -> 0.15988571529738219, short -> 0.2627166670059097, seeing -> 0.3575690010333115, cynics -> 0.5340640914451961, back -> 0.17926260049325804, sellers -> 0.4233201885888694, dwindling -> 0.43317342764614025, reuters -> 0.117255870884111, bears -> 0.3194568575554214, street -> 0.23379488513519717}                                                                                                                                                                                                                                                               |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2"
      ],
      "metadata": {
        "id": "TLtgC0Gd3rg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "R0tkPia13tCS",
        "outputId": "5b1786c0-5317-4bdd-925e-1795022f2d7b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   5035      0 --:--:-- --:--:-- --:--:--  5039\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0     97      0 --:--:-- --:--:-- --:--:--    98\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  39.7M      0  0:00:01  0:00:01 --:--:-- 39.7M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LOAD & PREPARE SVM DATA\n",
        "from pyspark.sql import functions as F\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Read Data\n",
        "feature_cols = [f\"f{i}\" for i in range(64)] + [\"label\"]\n",
        "svm_df = (spark.read\n",
        "              .csv(\"data_for_svm.csv\", header=False, inferSchema=True)\n",
        "              .toDF(*feature_cols))\n",
        "\n",
        "# Read w and b\n",
        "w_vec = pd.read_csv(\"w.csv\", header=None).values.flatten().astype(float)   # len = 64\n",
        "b_val = float(pd.read_csv(\"bias.csv\", header=None).iloc[0, 0])            # scalar\n",
        "\n",
        "w_bc = spark.sparkContext.broadcast(w_vec)\n",
        "b_bc = spark.sparkContext.broadcast(b_val)\n",
        "\n",
        "print(f\"Loaded {svm_df.count()} rows\")\n",
        "print(f\"w length  = {w_vec.shape[0]}  (should be 64)\")\n",
        "print(f\"b (bias)  = {b_val}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rdGKpRlb3u59",
        "outputId": "218ca225-6b3b-4520-9d78-d0700673b917"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 400000 rows\n",
            "w length  = 64  (should be 64)\n",
            "b (bias)  = 0.0001495661647902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q1\n",
        "# MapReduce functions required to calculate the loss function\n",
        "\n",
        "def map_hinge(row):\n",
        "    \"\"\"\n",
        "    Map step: for one (x_i, y_i) → (1, hinge_i)\n",
        "    hinge_i = max(0, 1 - y_i (w⋅x_i + b))\n",
        "    Uses the GLOBAL broadcasts w_bc, b_bc defined in Cell A.\n",
        "    \"\"\"\n",
        "    x = np.array([row[f\"f{i}\"] for i in range(64)], dtype=float)\n",
        "    y = float(row[\"label\"])\n",
        "    margin = y * (np.dot(w_bc.value, x) + b_bc.value)\n",
        "    hinge  = max(0.0, 1.0 - margin)\n",
        "    return (1, hinge)\n",
        "\n",
        "def red_sum(a, b):\n",
        "    \"\"\"Reduce step: add counts and hinge sums.\"\"\"\n",
        "    return (a[0] + b[0], a[1] + b[1])\n"
      ],
      "metadata": {
        "id": "aAmItCnQ4z-L"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2\n",
        "# Create loss function\n",
        "\n",
        "def loss_SVM(w, b, df, lmbda=1.0):\n",
        "    \"\"\"\n",
        "    Compute L(w,b) = λ‖w‖² + (1/n) Σ hinge_i  in pure MapReduce style.\n",
        "    \"\"\"\n",
        "    # fresh broadcasts for the given (w, b)\n",
        "    w_tmp = spark.sparkContext.broadcast(np.asarray(w, dtype=float))\n",
        "    b_tmp = spark.sparkContext.broadcast(float(b))\n",
        "\n",
        "    def hinge_only(row):\n",
        "        x = np.array([row[f\"f{i}\"] for i in range(64)], dtype=float)\n",
        "        y = float(row[\"label\"])\n",
        "        margin = y * (np.dot(w_tmp.value, x) + b_tmp.value)\n",
        "        return max(0.0, 1.0 - margin)\n",
        "\n",
        "    n   = df.count()\n",
        "    hinge_sum = df.rdd.map(hinge_only).sum()\n",
        "\n",
        "    reg_term  = lmbda * float(np.dot(w, w))\n",
        "    hinge_term = hinge_sum / n\n",
        "\n",
        "    return reg_term + hinge_term\n"
      ],
      "metadata": {
        "id": "kz9G6fej44zJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q3\n",
        "# Calculating the objective value\n",
        "\n",
        "loss_value = loss_SVM(w_vec, b_val, svm_df, lmbda=1.0)\n",
        "print(f\"SVM objective L(w,b) = {loss_value:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MMxEfe8y4-CW",
        "outputId": "a060844f-812f-4b7a-b99b-f5fbfc6bb080"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM objective L(w,b) = 1.002940\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4\n",
        "# Design the MapReduce function\n",
        "def map_predict(row):\n",
        "    x = np.array([row[f\"f{i}\"] for i in range(64)], dtype=float)\n",
        "    score = float(np.dot(w_bc.value, x) + b_bc.value)\n",
        "    pred  = 1 if score >= 0 else -1\n",
        "    return (pred,)\n",
        "\n",
        "# Run prediction across the data\n",
        "pred_rdd = svm_df.rdd.map(map_predict)\n",
        "pred_df  = pred_rdd.toDF([\"y_pred\"])\n",
        "\n",
        "svm_pred = svm_df.select(*feature_cols).withColumn(\"row_id\", F.monotonically_increasing_id()) \\\n",
        "                 .join(pred_df.withColumn(\"row_id\", F.monotonically_increasing_id()),\n",
        "                       on=\"row_id\").drop(\"row_id\")\n",
        "\n",
        "# Preview first 5 predictions\n",
        "svm_pred.select(\"label\", \"y_pred\").show(5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Fy-BTsFC6qmW",
        "outputId": "25fcc6d3-af1e-4f01-b752-78f7302d2b9b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+------+\n",
            "|label|y_pred|\n",
            "+-----+------+\n",
            "|   -1|    -1|\n",
            "|    1|    -1|\n",
            "|    1|    -1|\n",
            "|    1|     1|\n",
            "|   -1|    -1|\n",
            "+-----+------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}